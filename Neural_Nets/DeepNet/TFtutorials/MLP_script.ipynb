{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get MNIST dataset\n",
    "def get_mnist():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    \n",
    "    x_train, y_train = mnist[0].next_batch(55000)\n",
    "    x_valid, y_valid = mnist[1].next_batch(5000)\n",
    "    x_test, y_test = mnist[2].next_batch(10000)\n",
    "        \n",
    "    return ([x_train, y_train], [x_valid, y_valid], [x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_image(x, y, ix):\n",
    "    x = x[ix]\n",
    "    img = np.reshape(x, (28, 28))\n",
    "    y = y[ix]\n",
    "\n",
    "    plt.imshow(1-img, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    print np.where(y != 0)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural network class\n",
    "class neural_net:\n",
    "    def __init__(self, layers=[784, 10], activations=[tf.nn.softmax], error=tf.contrib.losses.log_loss):\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_inputs = layers[0]\n",
    "        self.n_outputs = layers[-1]\n",
    "        \n",
    "        self.x = tf.placeholder('float', [None, self.n_inputs], name='x')\n",
    "        self.y = tf.placeholder('float', [None, self.n_outputs], name='y')\n",
    "        \n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        self.neural_layers = [self.x]\n",
    "        self.trained_W = None\n",
    "        self.trained_B = None\n",
    "                \n",
    "        for layer in xrange(self.n_layers-1):\n",
    "            input_layer = self.neural_layers[-1]\n",
    "            \n",
    "            w = tf.Variable(tf.random_uniform([layers[layer], layers[layer+1]], name='rng_W_'+str(layer)), \n",
    "                            name='W_'+str(layer))\n",
    "            b = tf.Variable(tf.random_uniform([layers[layer+1]], name='rng_B_'+str(layer)), name='B_'+str(layer))\n",
    "            \n",
    "            output_layer = tf.add(tf.matmul(input_layer, w, name='W_'+str(layer)+'times_x'), \n",
    "                                  b, name='W_'+str(layer)+'_times_x_plus_B_'+str(layer))\n",
    "            self.neural_layers += [activations[layer](output_layer)]\n",
    "            \n",
    "            self.W += [w]\n",
    "            self.B += [b]\n",
    "            \n",
    "        self.error = error(self.neural_layers[-1], self.y)\n",
    "        \n",
    "    def train(self, train_x, train_y, step_size=0.01, batch_size=100, max_iters=1):\n",
    "        train_step = tf.train.AdamOptimizer(step_size, name='grad_desc').minimize(self.error)\n",
    "        \n",
    "        train_size, n_features = train_x.shape\n",
    "        train_x = np.vsplit(train_x, train_size/batch_size)\n",
    "        train_y = np.vsplit(train_y, train_size/batch_size)\n",
    "        \n",
    "        with tf.Session() as session:\n",
    "            tf.train.SummaryWriter('mlp_logs', session.graph)\n",
    "            session.run(tf.initialize_all_variables())\n",
    "            \n",
    "            for i in xrange(max_iters):\n",
    "                for batch_x, batch_y in zip(train_x, train_y):\n",
    "                    session.run(train_step, feed_dict={self.x: batch_x, self.y: batch_y})\n",
    "                    \n",
    "            W = session.run(self.W)\n",
    "            B = session.run(self.B)\n",
    "        \n",
    "        self.trained_W = W\n",
    "        self.trained_B = B\n",
    "        return (W, B)\n",
    "        \n",
    "    def predict(self, test_x):\n",
    "        with tf.Session() as session:\n",
    "            for i in xrange(self.n_layers-1):\n",
    "                if self.trained_W == None and self.trained_B == None:\n",
    "                    session.run(tf.initialize_all_variables())\n",
    "                else:\n",
    "                    session.run(self.W[i].assign(self.trained_W[i]))\n",
    "                    session.run(self.B[i].assign(self.trained_B[i]))\n",
    "            return session.run(self.neural_layers[-1], feed_dict={self.x: test_x})\n",
    "        \n",
    "    def unlearn(self):\n",
    "        self.trained_W = None\n",
    "        self.trained_B = None\n",
    "        with tf.Session() as session:\n",
    "            session.run(tf.initialize_all_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "[x_train, y_train], [x_valid, y_valid], [x_test, y_test] = get_mnist()\n",
    "\n",
    "def cross_entropy(predictions, labels):\n",
    "    log_pred = tf.log(predictions)\n",
    "    cross_entropy = -tf.mul(labels, log_pred)\n",
    "    return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "mlp = neural_net(layers=[784, 10], activations=[tf.nn.softmax], error=cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, B = mlp.train(x_train, y_train, max_iters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4939"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(y_pred, 1) != np.argmax(y_train, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "958"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlp.predict(x_test)\n",
    "np.sum(np.argmax(y_pred, 1) != np.argmax(y_test, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
